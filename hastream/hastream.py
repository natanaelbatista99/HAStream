import copy
import hdbscan
import math
import sys
import os
import typing
import time
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import networkx as nx
import torch.nn as nn

from river import base
from collections import defaultdict, deque
from sklearn.cluster import AgglomerativeClustering
from .micro_cluster import Vertex, MicroCluster
from .mutual_reachability_graph import MutualReachabilityGraph
from .minimal_spaning_tree import MinimalSpaningTree
from .updating import Updating
from .dendrogram import Dendrogram
from .evaluation import Evaluation
# parallelism
from multiprocessing import Pool, cpu_count

class HAStream(base.Clusterer, nn.Module):
    
    class BufferItem:
        def __init__(self, x, timestamp, covered):
            self.x         = x
            self.timestamp = (timestamp,)
            self.covered   = covered

    def __init__(
        self,
        mpts                   = [10],
        min_cluster_size       = 10,
        decaying_factor: float = 0.25,
        beta: float            = 0.75,
        mu: float              = 2,
        epsilon: float         = 0.02,
        n_samples_init: int    = 1000,
        stream_speed: int      = 100,
        m_movementThreshold    = 0.5,
        runtime                = False,
        plot                   = False,
        save_partitions        = False,
        percent                = 0.10,
        method_summarization   = 'single_linkage',
        dataset                = ''
    ):
        super().__init__()
        self.percent              = percent
        self.timestamp            = 0
        self.initialized          = False
        self.decaying_factor      = decaying_factor
        self.beta                 = beta
        self.mu                   = mu
        self.epsilon              = epsilon
        self.n_samples_init       = n_samples_init
        self.stream_speed         = stream_speed
        self.mst                  = None
        self.mst_mult             = None
        self.mpts                 = mpts
        self.min_cluster_size     = min_cluster_size
        self.m_movementThreshold  = m_movementThreshold
        self.runtime              = runtime
        self.plot                 = plot
        self.save_partitions      = save_partitions
        self.method_summarization = method_summarization
        self.base_dir_result      = os.path.join(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')), "results/" + str(dataset) + "/")
        self.dataset              = dataset
        
        # number of clusters generated by applying the variant of DBSCAN algorithm
        # on p-micro-cluster centers and their centers
        self.n_clusters = 0
        
        self.clusters: typing.Dict[int, "MicroCluster"]         = {}
        self.p_micro_clusters: typing.Dict[int, "MicroCluster"] = {}
        self.o_micro_clusters: typing.Dict[int, "MicroCluster"] = {}
        
        #mudei o método pq estava dando erro e não estavamos precisando no momento
        self._time_period = math.ceil((1 / self.decaying_factor) * math.log((self.mu * self.beta) / (self.mu * self.beta - 1))) + 1
        
        print("self._time_period", self._time_period)
        self._init_buffer: typing.Deque[typing.Dict] = deque()
        
        self._n_samples_seen = 0
        self.m_update        = None

        # DataFrame to save the runtimes
        if self.runtime:
            self.df_runtime_final      = pd.DataFrame(columns=['timestamp', 'micro_clusters', 'summarization', 'multiple_hierarchies'])

        if self.save_partitions:
            # DataFrame to save summarized objects from Data Bubbles
            self.df_mc_to_points = pd.DataFrame({
                "0": pd.Series(dtype="float"),
                "1": pd.Series(dtype="float"),
                "id_mc": pd.Series(dtype="Int64")
            })

        # check that the value of beta is within the range (0,1]
        if not (0 < self.beta <= 1):
            raise ValueError(f"The value of `beta` (currently {self.beta}) must be within the range (0,1].")

    @property
    def centers(self):
        return {k: cluster.calc_center(self.timestamp) for k, cluster in self.clusters.items()}

    @staticmethod
    def _distance(point_a, point_b):
        square_sum = 0
        dim        = len(point_a)
        
        for i in range(dim):
            square_sum += math.pow(point_a[i] - point_b[i], 2)
        
        return math.sqrt(square_sum)
    
    def distanceEuclidian(self, x1, x2):
        distance = 0
        
        for i in range(len(x1)):
            d         = x1[i] - x2[i]
            distance += d * d
            
        return math.sqrt(distance)

    def _get_closest_cluster_key(self, point, clusters):
        min_distance = math.inf
        key          = -1
        
        for k, cluster in clusters.items():
            distance = self.distanceEuclidian(cluster.getCenter(self.timestamp), point)
            
            if distance < min_distance and distance <= self.epsilon:
                min_distance = distance
                key          = k
                
        return key

    def _merge(self, point):
        # initiate merged status
        merged_status = False

        pos = self._n_samples_seen - 1

        if self.save_partitions:
            self.df_mc_to_points.loc[pos, '0']     = point[0]
            self.df_mc_to_points.loc[pos, '1']     = point[1]
            self.df_mc_to_points.loc[pos, 'id_mc'] = 0

        if len(self.p_micro_clusters) != 0:
            # try to merge p into its nearest p-micro-cluster c_p
            closest_pmc_key = self._get_closest_cluster_key(point, self.p_micro_clusters)
            
            if closest_pmc_key != -1:
                updated_pmc = copy.deepcopy(self.p_micro_clusters[closest_pmc_key])
                updated_pmc.insert(point, self.timestamp)
                
                if updated_pmc.getRadius(self.timestamp) <= self.epsilon:
                    # keep updated p-micro-cluster
                    self.p_micro_clusters[closest_pmc_key] = updated_pmc

                    if self.save_partitions:
                        self.df_mc_to_points.loc[pos, 'id_mc'] = closest_pmc_key

                    merged_status = True

        if not merged_status:
            closest_omc_key = self._get_closest_cluster_key(point, self.o_micro_clusters)
            
            if closest_omc_key != -1:
                updated_omc = copy.deepcopy(self.o_micro_clusters[closest_omc_key])
                updated_omc.insert(point, self.timestamp)

                if updated_omc.getRadius(self.timestamp) <= self.epsilon:
                    # keep updated o-micro-cluster
                    weight_omc = updated_omc.getWeight(self.timestamp)
                    
                    if weight_omc > self.mu * self.beta:
                        # it has grown into a p-micro-cluster
                        del self.o_micro_clusters[closest_omc_key]

                        new_key = 0
                        
                        while new_key in self.p_micro_clusters:
                            new_key += 1
                            
                        updated_omc.setID(new_key)
                        self.p_micro_clusters[new_key] = updated_omc

                        if self.save_partitions:
                            self.df_mc_to_points.loc[pos, 'id_mc'] = new_key
                            self.df_mc_to_points['id_mc']          = self.df_mc_to_points['id_mc'].replace((-1) * closest_omc_key, new_key)
                            
                    else:
                        self.o_micro_clusters[closest_omc_key] = updated_omc

                        # Outliers have our key negative
                        if self.save_partitions:
                            self.df_mc_to_points.loc[pos, 'id_mc'] = (-1) * closest_omc_key
                    
                    merged_status = True
                    
        if not merged_status:
            # create a new o-data_bubble by p and add it to o_data_bubbles
            mc_from_p = MicroCluster(x=point, timestamp=self.timestamp, decaying_factor=self.decaying_factor)

            key_o = 2

            while key_o in self.o_micro_clusters:
                key_o += 1

            self.o_micro_clusters[key_o] = mc_from_p

            if self.save_partitions:
                self.df_mc_to_points.loc[pos, 'id_mc'] = (-1) * key_o

            merged_status = True
    
    def _merge_old(self, point):
        # initiate merged status
        merged_status = False
        
        pos = self._n_samples_seen - 1

        if len(self.p_micro_clusters) != 0:
            # try to merge p into its nearest p-micro-cluster c_p
            closest_pmc_key = self._get_closest_cluster_key(point, self.p_micro_clusters)
            
            if closest_pmc_key != -1:
                updated_pmc     = copy.deepcopy(self.p_micro_clusters[closest_pmc_key])
                updated_pmc.insert(point, self.timestamp)

                if updated_pmc.getRadius(self.timestamp) <= self.epsilon:
                    # keep updated p-micro-cluster
                    self.p_micro_clusters[closest_pmc_key] = updated_pmc
                    merged_status = True

                    self.df_mc_to_points.loc[pos, 'id_mc'] = closest_pmc_key

                    if self.p_micro_clusters[closest_pmc_key].hasCenterChanged(self.m_movementThreshold, self.epsilon, self.timestamp):
                        self.p_micro_clusters[closest_pmc_key].setStaticCenter(self.timestamp)

        if not merged_status and len(self.o_micro_clusters) != 0:
            
            closest_omc_key = self._get_closest_cluster_key(point, self.o_micro_clusters)
            
            if closest_omc_key != -1:
                updated_omc     = copy.deepcopy(self.o_micro_clusters[closest_omc_key])
                updated_omc.insert(point, self.timestamp)

                if updated_omc.getRadius(self.timestamp) <= self.epsilon:
                    # keep updated o-micro-cluster
                    if updated_omc.getWeight(self.timestamp) > self.mu * self.beta:
                        # it has grown into a p-micro-cluster
                        del self.o_micro_clusters[closest_omc_key]
                        updated_omc.setStaticCenter(self.timestamp)

                        new_key = 0

                        while new_key in self.p_micro_clusters:
                            new_key += 1

                        updated_omc.setID(new_key)
                        self.p_micro_clusters[new_key] = updated_omc

                        self.df_mc_to_points.loc[pos, 'id_mc'] = new_key
                        self.df_mc_to_points['id_mc']          = self.df_mc_to_points['id_mc'].replace((-1) * closest_omc_key, new_key)

                    else:
                        self.o_micro_clusters[closest_omc_key] = updated_omc                    

                        # Outliers have our key negative
                        self.df_mc_to_points.loc[pos, 'id_mc'] = (-1) * closest_omc_key

                    merged_status = True
                
        if not merged_status:
            # create a new o-data_bubble by p and add it to o_micro_clusters
            mc_from_p = MicroCluster(x=point, timestamp=self.timestamp, decaying_factor=self.decaying_factor)

            key_o = 2

            while key_o in self.o_micro_clusters:
                key_o += 1

            self.o_micro_clusters[key_o] = mc_from_p

            self.df_mc_to_points.loc[pos, 'id_mc'] = (-1) * key_o

            merged_status = True

        # when p is not merged and o-micro-cluster set is empty
        # if not merged_status and len(self.o_micro_clusters) == 0:
        #    mc_from_p = MicroCluster(x=point, timestamp=self.timestamp, decaying_factor=self.decaying_factor)
        #    self.o_micro_clusters = {2: mc_from_p}
        #    self.df_mc_to_points.loc[pos, 'id_mc'] = -2
        #    merged_status = True
            
    def _is_directly_density_reachable(self, c_p, c_q):
        if c_p.calc_weight() > self.mu and c_q.calc_weight() > self.mu:
            # check distance of two clusters and compare with 2*epsilon
            c_p_center = c_p.calc_center(self.timestamp)
            c_q_center = c_q.calc_center(self.timestamp)
            distance   = self._distance(c_p_center, c_q_center)
            
            if distance < 2 * self.epsilon and distance <= c_p.calc_radius() + c_q.calc_radius():
                return True
        return False

    def _query_neighbor(self, cluster):
        neighbors = deque()
        # scan all clusters within self.p_micro_clusters
        for pmc in self.p_micro_clusters.values():
            # check density reachable and that the cluster itself does not appear in neighbors
            if cluster != pmc and self._is_directly_density_reachable(cluster, pmc):
                neighbors.append(pmc)
        return neighbors

    @staticmethod
    def _generate_clusters_for_labels(cluster_labels):
        # initiate the dictionary for final clusters
        clusters = {}

        # group clusters per label
        mcs_per_label = defaultdict(deque)
        for mc, label in cluster_labels.items():
            mcs_per_label[label].append(mc)

        # generate set of clusters with the same label
        for label, micro_clusters in mcs_per_label.items():
            # merge clusters with the same label into a big cluster
            cluster = copy.copy(micro_clusters[0])
            for mc in range(1, len(micro_clusters)):
                cluster.merge(micro_clusters[mc])

            clusters[label] = cluster

        return len(clusters), clusters

    def _build(self):

        start_time_total = time.time()

        self.time_period_check()
        
        print("\n>> Timestamp: ", self.timestamp)
        print("> count_potential", len(self.p_micro_clusters))
        print("> count_outlier", len(self.o_micro_clusters))
        
        if len(self.p_micro_clusters) < (max(self.mpts) / self.mu):
            print("no building possible since num_potential_mcs < Mpts")
            return

        # MULTIPLE HIERARCHIES -------------------------------------------------------------------------------------------
        if self.save_partitions:
            self.remove_oldest_points_in_micro_clusters_timestamp()
            self.micro_clusters_to_points(self.timestamp)
        
        print("Computing Multiple Hierarchies:")
        start_hierarchies = time.time()

        # PARALLELISM
        try: 
            args = [mptsi for mptsi in self.mpts]

            with Pool(processes = (cpu_count() - 10)) as pool: 
                results = pool.map(self.compute_hierarchy_mpts, args)
        except KeyboardInterrupt:
            print("Interrompido pelo usuário")

        print(">Time Total: ", time.time() - start_time_total)
        
        if self.save_partition:
            # ASSESSMENT
            evaluation = Evaluation(self.dataset, self.mpts, self.timestamp)
            evaluation.evaluation_mensure()

        if self.runtime:
            self.save_runtime_timestamp(results)

            # Time final timestamp
            self.df_runtime_final.at[self.timestamp, 'timestamp']            = self.timestamp
            self.df_runtime_final.at[self.timestamp, 'micro_clusters']       = len(self.p_micro_clusters)
            self.df_runtime_final.at[self.timestamp, 'multiple_hierarchies'] = time.time() - start_hierarchies
    
    def compute_hierarchy_mpts(self, mptsi):

        if self.save_partitions:
            df_partition = self.df_mc_to_points[(self.df_mc_to_points['id_mc'] != -1)]
            len_points   = df_partition.shape[0]
            len_mcs      = len(self.p_micro_clusters)
            
            # OBJECTS PARTITION
            partition_mcs_o     = [-1 for j in range(len_points)]
            partition_hdbscan_o = [-1 for j in range(len_points)]

        print("\n-------------------------------------------------------------------------------------")
        print("Mpts: ", mptsi)
        
        start_time_mptsi = time.time()

        Vertex.s_idCounter = 0
    
        for mc in self.p_micro_clusters.values():
            mc.setVertexRepresentative(None)
            mc.setStaticCenter(self.timestamp)
        
        G = nx.Graph()
    
        start_mrg = time.time()
        mrg       = MutualReachabilityGraph(G, self.p_micro_clusters.values(), mptsi, self.timestamp)
        mrg.buildGraph()
        end_mrg   = time.time()
        print("> Time for MRG: ", end_mrg - start_mrg)
        
        start_mst = time.time()
        T         = nx.minimum_spanning_tree(G)
        mst       = MinimalSpaningTree(T)
        mst.buildGraph()
        end_mst   = time.time()
        print("> Time for MST: ", end_mst - start_mst)
        
        self.m_update = Updating(mrg, mst)
        
        end_time_mptsi = time.time()
        print("> Total Time MPts: ", end_time_mptsi - start_time_mptsi)

        start_selection = end_selection = start_dendrogram = end_dendrogram = 0

        # Time Selection CLusters
        if self.save_partitions:
            start_dendrogram = time.time()
            dendrogram       = Dendrogram(mst, mptsi, self.timestamp)
            dendrogram.build()
            end_dendrogram   = time.time()
            print("> Time for Dendrogram: ", end_time_mptsi - start_time_mptsi)

            start_selection = time.time()           
            selection       = dendrogram.clusterSelection()
            partition       = [-1 for j in range(len_mcs + 10)]

            # Partitions HAStream Mptsi
            cont          = 1
            partition_mcs = {}
        
            for n in selection:
                it = iter(n.getVertices())

                for el in it:
                    partition_mcs[el.getMicroCluster().getID()] = cont
                    partition[el.getID()]                       = cont

                cont += 1

            cont = 0
            for i, row in df_partition.iterrows():
                if row['id_mc'] in partition_mcs:
                    partition_mcs_o[cont] = partition_mcs[row['id_mc']]
                cont += 1
        
            end_selection = time.time()

            self.save_partition(partition, mst.getVertices(), mptsi)

            # Partitions HDBSCAN
            start_hdbscan = time.time()
            
            clusterer = hdbscan.HDBSCAN(min_cluster_size = 10, min_samples = mptsi, match_reference_implementation = True, core_dist_n_jobs = 1)
            clusterer.fit(df_partition.drop("id_mc", axis=1))
            labels    = clusterer.labels_

            p = 0
            for i in labels:
                partition_hdbscan_o[p] = i
                p += 1
                
            end_hdbscan = time.time()
            print(">Time for HDBSCAN: ", end_hdbscan - start_hdbscan)

            # Plot
            if self.plot:
                self.plot_partition(partition, mptsi, df_partition)
                self.plot_hdbscan_result(mptsi, labels, df_partition)

            # SAVING OBJECTS PARTITIONS
            self.save_partition_mcs_and_objects_mpts(partition_mcs_o, partition_hdbscan_o, mptsi)
        
        if self.runtime:
            df_runtime_timestamps                   = {}
            df_runtime_timestamps['mpts']           = mptsi
            df_runtime_timestamps['micro_clusters'] = len(self.p_micro_clusters)
            df_runtime_timestamps['mrg']            = (end_mrg - start_mrg)
            df_runtime_timestamps['mst']            = (end_mst - start_mst)
            df_runtime_timestamps['dendrogram']     = (end_dendrogram - start_dendrogram)
            df_runtime_timestamps['selection']      = (end_selection - start_selection) 
            df_runtime_timestamps['total']          = (end_time_mptsi - start_time_mptsi)
            
            return df_runtime_timestamps
            
        G = mrg = mst = mst_max = dendrogram = selection = clusterer = None
        
    def _initial_single_linkage(self):
        start = time.time()
        
        self._init_buffer = np.array(self._init_buffer)
        
        # The linkage="single" does a clustering, e. g., the clusters are indentified and form big data bubbles.
        clustering = AgglomerativeClustering(n_clusters = int(self.n_samples_init * self.percent), linkage='average')
        clustering.fit(self._init_buffer)
        
        labels          = clustering.labels_
        labels_visited  = np.zeros(len(labels))
        len_buffer      = len(self._init_buffer)
        count_potential = 0
        min_mc = max_mc = 0
        epsilon         = {}
        pos_point       = 0

        if self.save_partition:
            mc_to_points = []
        
        for i in range(len_buffer):
            
            label      = labels[i]
            object_new = dict(enumerate(self._init_buffer[i]))
            labels_visited[label] += 1
            
            if labels_visited[label] == 1:
                mc = MicroCluster(
                    x = object_new,
                    timestamp = self.timestamp,
                    decaying_factor = self.decaying_factor,
                )

                mc.setID(label)
                
                self.p_micro_clusters.update({label: mc})
                
            else:
                self.p_micro_clusters[label].insert(object_new, self.timestamp)

                if labels_visited[label] == self.mu:
                    count_potential += 1
                if self.p_micro_clusters[label].getN() >= self.mu:
                    epsilon[label] = self.p_micro_clusters[label].getRadius(self.timestamp)
                
            max_mc = max(max_mc, self.p_micro_clusters[label].getN())

            if self.save_partitions:
                mc_to_points.append([self._init_buffer[i][0], self._init_buffer[i][1], label])
                pos_point += 1
        
        if self.save_partitions:
            self.df_mc_to_points = pd.DataFrame(mc_to_points, columns=['0', '1', 'id_mc'])
        
        # outliers data_bubbles
        if count_potential != len(self.p_micro_clusters):
            replace_map = {}
            key         = 0
            key_p       = 0
            key_o       = 2
            
            while labels_visited[key]:
                if labels_visited[key] < self.mu:
                    if self.save_partition:
                        replace_map[key] = (-1) * key_o
                    
                    self.o_micro_clusters[key_o] = self.p_micro_clusters[key]
                    self.p_micro_clusters.pop(key)

                    key_o += 1
                else:
                    if key != key_p:
                        self.p_micro_clusters[key].setID(key_p)
                        self.p_micro_clusters[key_p] = self.p_micro_clusters.pop(key)

                        if min_mc == 0:
                            min_mc = self.p_micro_clusters[key_p].getN()
                        else:
                            min_mc = min(min_mc, self.p_micro_clusters[key_p].getN())
                        
                        if self.save_partitions:
                            #update new key
                            replace_map[key] = key_p

                    key_p += 1
                key += 1

            if self.save_partitions:
                self.df_mc_to_points['id_mc'] = self.df_mc_to_points['id_mc'].replace(replace_map)

        end = time.time()
        
        # Time
        if self.runtime:
            self.df_runtime_final.at[self.timestamp, 'summarization'] = end - start
        
        e_min  = min(epsilon.values())
        e_mean = sum(epsilon.values()) / count_potential
        e_max  = max(epsilon.values())
        
        self.epsilon = e_max
        
        print("> Total: ", (count_potential + len(self.o_micro_clusters)))
        print("> MCs Potential: ", count_potential)
        print("> Min_MC: ", min_mc)
        print("> Max_MC: ", max_mc)
        print("> Time for MCs: ", end - start)
        print("> Epsilon min: ", e_min)
        print("> Epsilon mean: ", e_mean)
        print("> Epsilon max: ", e_max)
        
    def _expand_cluster(self, mc, neighborhood, id):
        for idx in neighborhood:
            item = self._init_buffer[idx]
            
            if not item.covered:
                item.covered = True
                mc.add(item.x)

                p = self.df_mc_to_points[(self.df_mc_to_points['x'] == item.x[0]) & (self.df_mc_to_points['y'] == item.x[1])]
                self.df_mc_to_points.at[p.index[0], 'id_mc'] = id

    def _get_neighborhood_ids(self, item):
        neighborhood_ids = deque()
        
        for idx, other in enumerate(self._init_buffer):
            if not other.covered:
                if self._distance(item.x, other.x) < self.epsilon:
                    neighborhood_ids.append(idx)
                    
        return neighborhood_ids
    
    def _initial_epsilon(self):
        start = time.time()
        
        for item in self._init_buffer:
            if not item.covered:
                item.covered = True
                neighborhood = self._get_neighborhood_ids(item)
                
                if len(neighborhood) > self.mu:
                    mc = MicroCluster(
                        x = item.x,
                        timestamp = self.timestamp,
                        decaying_factor = self.decaying_factor,
                    )
                    
                    id = len(self.p_micro_clusters)
                    
                    p = self.df_mc_to_points[(self.df_mc_to_points['x'] == item.x[0]) & (self.df_mc_to_points['y'] == item.x[1])]
                    self.df_mc_to_points.at[p.index[0], 'id_mc'] = id
                    
                    self._expand_cluster(mc, neighborhood, id)
                    mc.setStaticCenter(self.timestamp)
                    mc.setID(id)
                    
                    self.p_micro_clusters.update({id: mc})
                else:
                    item.covered = False

        # Outliers
        for item in self._init_buffer:
            if not item.covered:
                item.covered = True
                neighborhood = self._get_neighborhood_ids(item)
                
                mc = MicroCluster(x = item.x, timestamp = self.timestamp, decaying_factor = self.decaying_factor)

                id = 2 if len(self.o_micro_clusters) == 0 else list(self.o_micro_clusters.keys())[-1] + 1
                
                p = self.df_mc_to_points[(self.df_mc_to_points['x'] == item.x[0]) & (self.df_mc_to_points['y'] == item.x[1])]
                self.df_mc_to_points.at[p.index[0], 'id_mc'] = -id
                
                self._expand_cluster(mc, neighborhood, -id)
                mc.setStaticCenter(self.timestamp)
                mc.setID(id)
                
                self.o_micro_clusters.update({id: mc})
                
        end = time.time()
        
        # Time
        if self.runtime:
            self.df_runtime_final.at[self.timestamp, 'summarization'] = end - start
        
        print(">> Time Initial Summartization: ", end - start)
        
    def time_period_check(self):
        for i, p_micro_cluster_i in list(self.p_micro_clusters.items()):
            if p_micro_cluster_i.getWeight(self.timestamp) < self.mu * self.beta:
                # c_p became an outlier and should be deleted
                del self.p_micro_clusters[i]

        for j, o_micro_cluster_j in list(self.o_micro_clusters.items()):
            # calculate xi
            xi = (2**(-self.decaying_factor * (self.timestamp - o_micro_cluster_j.creation_time + self._time_period)) - 1) / (2 ** (-self.decaying_factor * self._time_period) - 1)                
            if o_micro_cluster_j.getWeight(self.timestamp) < xi:
                # c_o might not grow into a p-micro-cluster, we can safely delete it
                self.o_micro_clusters.pop(j)

    def learn_one(self, x, sample_weight=None):
        self._n_samples_seen += 1
        # control the stream speed
        
        if self._n_samples_seen % self.stream_speed == 0:
            self.timestamp += 1

        # Initialization
        if not self.initialized:
            if self.method_summarization == 'epsilon':
                self._init_buffer.append(self.BufferItem(x, self.timestamp, False))
            else:
                self._init_buffer.append(list(x.values()))
            
            if len(self._init_buffer) == self.n_samples_init:
                print("entrando no initial()")
                if self.method_summarization == 'epsilon':
                    self._initial_epsilon()
                else:
                    self._initial_single_linkage()
                
                self.initialized = True
                del self._init_buffer
                
            return self

        # Merge
        self._merge(x)

        # Periodic cluster removal
        if self.timestamp > 0 and self.timestamp % self._time_period == 0:
            self.time_period_check()

        return self

    def predict_one(self, sample_weight=None):        
        # This function handles the case when a clustering request arrives.
        # implementation of the DBSCAN algorithm proposed by Ester et al.
        
        if not self.initialized:
            return 0
        
        self._build()
    
    def save_partition(self, partition, vertices, mpts):
        m_directory = os.path.join(self.base_dir_result, "flat_solutions")
        
        try:
            sub_dir = os.path.join(m_directory, "flat_solution_partitions_t" + str(self.timestamp) + "/partitions_mcs")

            if not os.path.exists(sub_dir):
                os.makedirs(sub_dir)

            # SAVE MPTS PARTITION
            df_partition                  = pd.DataFrame([partition])
            df_partition['partition_mpts'] = mpts
            df_partition.columns          = df_partition.columns.map(str)
            df_partition.to_parquet(os.path.join(sub_dir, f"partitions_mpts_{mpts}.parquet"), index=True)
            
            if self.plot:
                cores = ["blue", "red", "orange", "green", "purple", "brown", "pink", "olive", "cyan"]
                
                with open(os.path.join(sub_dir, "partition_plot_mpts_" + str(mpts) + ".csv"), 'w') as writer:
                    writer.write("x,y,N,radio,color,cluster,ID\n")

                    for v in vertices:
                        if partition[v.getID()] == -1:
                            writer.write(str(v.getMicroCluster().getCenter(self.timestamp)[0]) + "," + str(v.getMicroCluster().getCenter(self.timestamp)[1]) + "," + str(v.getMicroCluster().getWeight(self.timestamp)) + "," + str(v.getMicroCluster().getRadius(self.timestamp)) + ",black,-1," + str(v.getMicroCluster().getID()) + "\n")
                        else:
                            writer.write(str(v.getMicroCluster().getCenter(self.timestamp)[0]) + "," + str(v.getMicroCluster().getCenter(self.timestamp)[1]) + "," + str(v.getMicroCluster().getWeight(self.timestamp)) + "," + str(v.getMicroCluster().getRadius(self.timestamp)) + "," + cores[partition[v.getID()] % 9] + "," + str(partition[v.getID()]) + "," + str(v.getMicroCluster().getID()) + "\n")

        except FileNotFoundError as e:
            print(e)
            
    def plot_partition(self, partition, mpts, df_partition):
        sns.set_context('poster')
        sns.set_style('white')
        sns.set_color_codes()
        
        plot_kwds = {'s' : 2, 'linewidths':0}
        
        m_directory = os.path.join(self.base_dir_result, "plots")
        
        try:
            sub_dir = os.path.join(m_directory, "plot_mcs_t" + str(self.timestamp))

            if not os.path.exists(sub_dir):
                os.makedirs(sub_dir)
                
            partition = pd.read_csv(str(self.base_dir_result) + 'flat_solutions/flat_solution_partitions_t' + str(self.timestamp) + '/partitions_mcs/partition_plot_mpts_' + str(mpts) + '.csv', sep=',')

            # Statistic partition-------------------------------
            count_outlier = 0
            count_cluster = 0

            for j in range(len(partition)):

                if(partition['cluster'].loc[j] == -1):
                    count_outlier += 1

                if(partition['cluster'].loc[j] > count_cluster):
                    count_cluster = partition['cluster'].loc[j]

            legend  = ""
            legend += "Mpts: " + str(mpts) + "  "
            legend += "| Outliers: " + str(int((count_outlier * 100.0) / len(partition))) + "%  "
            legend += "| Clusters: " + str(count_cluster) + "  "
            legend += "| MCs: " + str(len(partition)) + "  "
            legend += "| Timestamp: " + str(self.timestamp)
            # -------------------------------------------------

            plt.figure(figsize = (16,12))

            for j in range(len(partition)):        
                plt.gca().add_patch(plt.Circle((partition['x'].loc[j], partition['y'].loc[j]), partition['radio'].loc[j], color=partition['color'].loc[j], fill=False))

            plt.scatter(df_partition['0'], df_partition['1'], c='black', **plot_kwds, label=legend)
            plt.legend(bbox_to_anchor=(-0.1, 1.02, 1, 0.2), loc="lower left", borderaxespad=0, fontsize=28)
            plt.savefig(str(self.base_dir_result) + "plots/plot_mcs_t" + str(self.timestamp) + "/mpts_" + str(mpts) + ".png")
            plt.close()
                
        except FileNotFoundError as e:
            print(e)

    def plot_hdbscan_result(self, mpts, labels, df_partition):
        m_directory = os.path.join(self.base_dir_result, "plots")
        sub_dir     = os.path.join(m_directory, "plot_mcs_t" + str(self.timestamp))

        if not os.path.exists(sub_dir):
            os.makedirs(sub_dir)
        
        sns.set_context('poster')
        sns.set_style('white')
        sns.set_color_codes()
        
        plot_kwds = {'s' : 10, 'linewidths':0}

        plt.figure(figsize = (16,12))
        title  = ""
        title += "HDBSCAN Mpts: " + str(mpts) + " | "
        title += "Clusters: " + str(len(set(labels)) - (1 if -1 in labels else 0)) + " | "
        title += "Outliers: " + str(np.sum(labels == -1)) + " | "
        title += "Len Points: " + str(len(labels))
        
        plt.title(title)
        plt.scatter(df_partition['0'], df_partition['1'], c=labels, cmap='magma', **plot_kwds)
        plt.savefig(str(self.base_dir_result) + "plots/plot_mcs_t" + str(self.timestamp) + "/mpts_" + str(mpts) + "_hdbscan.png")
        plt.close('all')

    def save_partition_mcs_and_objects_mpts(self, partition_mcs_o, partition_hdbscan_o, mpts):

        m_directory = os.path.join(self.base_dir_result, "flat_solutions")
        
        try:
            sub_dir = os.path.join(m_directory, "flat_solution_partitions_t" + str(self.timestamp) + '/partitions_objects')

            if not os.path.exists(sub_dir):
                os.makedirs(sub_dir)

            df_partition_mcs_o                     = pd.DataFrame.from_dict([partition_mcs_o])
            df_partition_mcs_o['partition_mpts']   = mpts
            df_partition_mcs_o.columns             = df_partition_mcs_o.columns.map(str)
            df_partition_mcs_o.to_parquet(os.path.join(sub_dir, f"partition_mcs_mpts_{mpts}.parquet"), index=True)

            df_partition_hdbscan_o                   = pd.DataFrame.from_dict([partition_hdbscan_o])
            df_partition_hdbscan_o['partition_mpts'] = mpts
            df_partition_hdbscan_o.columns           = df_partition_hdbscan_o.columns.map(str)
            df_partition_hdbscan_o.to_parquet(os.path.join(sub_dir, f"partition_hdbscan_mpts_{mpts}.parquet"), index=True)

        except FileNotFoundError as e:
            print(e)
    
    def save_runtime_timestamp(self, results):
        m_directory = os.path.join(self.base_dir_result, "runtime")
        
        try:
            if not os.path.exists(m_directory):
                os.makedirs(m_directory)

            df_runtime_timestamps = pd.DataFrame(results)

            with open(os.path.join(m_directory, "runtime_t" + str(self.timestamp) + ".csv"), 'w') as writer:
                writer.write("mpts,mrg,mst,dendrogram,selection,total\n")

                for _, linha in df_runtime_timestamps.iterrows():
                    writer.write(str(linha['mpts']) + ',' + str(linha['mrg']) + ',' + str(linha['mst']) + ',' + str(linha['dendrogram']) + ',' + str(linha['selection']) + ',' + str(linha['total']) + "\n")

        except FileNotFoundError as e:
            print(e)
            
    def save_runtime_final(self):
        m_directory = os.path.join(self.base_dir_result, "runtime")
        
        try:
            if not os.path.exists(m_directory):
                os.makedirs(m_directory)

            with open(os.path.join(m_directory, "runtime_final_t" + str(self.timestamp) + ".csv"), 'w') as writer:
                writer.write("timestamp,micro_clusters,summarization,multiple_hierarchies\n")

                for _, linha in self.df_runtime_final.iterrows():
                    writer.write(str(linha['timestamp']) + ',' + str(linha['micro_clusters']) + ',' + str(linha['summarization']) + ',' + str(linha['multiple_hierarchies']) + "\n")

        except FileNotFoundError as e:
            print(e)
    
    def remove_oldest_points_in_micro_clusters_timestamp(self):
        # Remove oldest objects from removed MCs
        for i, row in self.df_mc_to_points.iterrows():
            if i <= self._n_samples_seen:
                if row['id_mc'] not in self.p_micro_clusters and row['id_mc'] > -1:
                    self.df_mc_to_points.at[i, 'id_mc'] = -1
                elif ((-1) * row['id_mc']) not in self.o_micro_clusters and row['id_mc'] < -1:
                    self.df_mc_to_points.at[i, 'id_mc'] = -1
        
        # Remove oldest objects inside of MCs
        max_id         = max(self.p_micro_clusters.keys())
        labels_visited = np.zeros(max_id + 1)
        diff_points    = np.zeros(max_id + 1)
        
        for i in range(self._n_samples_seen):
            id_mc = self.df_mc_to_points.loc[i, 'id_mc']
            
            if id_mc > -1:
                if not labels_visited[id_mc] and id_mc in self.p_micro_clusters:
                    labels_visited[id_mc] = self.df_mc_to_points[self.df_mc_to_points['id_mc'] == id_mc].shape[0]
                    n = self.p_micro_clusters[id_mc].getWeight(self.timestamp)
                    
                    diff_points[id_mc] = int(labels_visited[id_mc] - n)
                    
                if diff_points[id_mc]:
                    diff_points[id_mc] -= 1
                    self.df_mc_to_points.loc[i, 'id_mc'] = -1
                    
        del labels_visited
        del diff_points 
        
        return self.df_mc_to_points[self.df_mc_to_points['id_mc'] != -1]
    
    def micro_clusters_to_points(self, timestamp):

        m_directory = os.path.join(self.base_dir_result, "datasets")
        
        try:
            if not os.path.exists(m_directory):
                os.makedirs(m_directory)
            
            df_filtered         = self.df_mc_to_points[(self.df_mc_to_points['id_mc'] != -1)]
            df_filtered.columns = df_filtered.columns.map(str)
            df_filtered.to_parquet(str(m_directory) + '/data_t' + str(self.timestamp) + '.parquet', index=False)

            if self.plot:
                sns.set_context('poster')
                sns.set_style('white')
                sns.set_color_codes()

                plot_kwds = {'s' : 1, 'linewidths':0}

                plt.figure(figsize=(12, 10))

                for key, value in self.p_micro_clusters.items():
                    plt.gca().add_patch(plt.Circle((value.getCenter(timestamp)[0], value.getCenter(timestamp)[1]), value.getRadius(timestamp), color='red', fill=False))

                for key, value in self.o_micro_clusters.items():
                    plt.gca().add_patch(plt.Circle((value.getCenter(timestamp)[0], value.getCenter(timestamp)[1]), value.getRadius(timestamp), color='blue', fill=False))

                plt.title("Timestamp: " + str(self.timestamp) + " | # Points: " + str(df_filtered.shape[0]) + " | # MCs: " + str(len(self.p_micro_clusters)), fontsize=20)
                plt.scatter(df_filtered['0'], df_filtered['1'], c='green', **plot_kwds)
                plt.savefig(str(self.base_dir_result) + "datasets/plot_dataset_t" + str(self.timestamp) + ".png")
                plt.close()
            
        except FileNotFoundError as e:
            print(e)